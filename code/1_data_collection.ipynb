{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection \n",
    "\n",
    "Author: Preey Sawadmanod\n",
    "\n",
    "---\n",
    "\n",
    "The goal of our project was to map cities affected by Hurricane Sandy along the US East Coast and identify areas where survivors may have needed assistance. The first step was to gather data through web scraping through social media, however there were some data acquisition limitations. For instance, Facebook and Instagram have very restrictive APIs due to security issues, so these options were not used. \n",
    "\n",
    "One way we collected data was with the Twitter API. Twitter has opt-in geo-tagging of posts, but there are not many users who enable the feature, so we used their API to target specific users. For example we targeted tweets from  SandyAid, a non-profit organization founded in New York City that offered help during Hurricane Sandy, and we assumed that their location was New York City. \n",
    "\n",
    "TwitterScaper is another package we used to scrape historical tweets. It features a geocode which is assigned to the nearest city of the tweet. We used this feature to filter tweets gathered to within a certain radius of chosen cities along the East Coast, sent during the time period of the hurricane, and containing select keywords. \n",
    "\n",
    "We used both Twitter API TwitterScraper to collect data and combined all the data gathered into a single data frame. \n",
    "\n",
    "## Table of Contents \n",
    "---\n",
    "- [Import packages](#Import-packages)\n",
    "- [Twitter API](#Twitter-API)\n",
    "- [TwitterScraper](#TwitterScraper)\n",
    "- [Combined tweets](#Combined-tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TwitterScraper is a separate package and can be install with the code below\n",
    "# !pip install twitterscraper\n",
    "# !pip install tweepy\n",
    "# !pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import miscellaneous\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Load packages from tweepy \n",
    "import tweepy\n",
    "from tweepy               import Stream\n",
    "from tweepy               import OAuthHandler\n",
    "from tweepy.streaming     import StreamListener\n",
    "from unidecode            import unidecode\n",
    "\n",
    "#Load packges from twitterscraper\n",
    "import twitterscraper\n",
    "from twitterscraper       import query_tweets\n",
    "\n",
    "#Load additional packages \n",
    "import json\n",
    "import sqlite3\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "import pandas   as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API \n",
    "\n",
    "Scraping tweets with Twitter API to specifically target users in volunteer organization in New York City. \n",
    "API keys are not included, and therefore need to be generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get tweets via API provided by Nick Read\n",
    "def get_all_tweets(screen_name):\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = OAuthHandler(ckey, csecret)\n",
    "    auth.set_access_token(atoken, asecret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "      #Add API keys here\n",
    "#     ckey    = \n",
    "#     csecret =\n",
    "#     atoken  =\n",
    "#     asecret = \n",
    "    \n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "    \n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "    \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print (\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "    #all subsequent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "    \n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print (\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "    outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")] for tweet in alltweets]\n",
    "    \n",
    "    #write the csv\t\n",
    "    with open('%s_tweets.csv' % screen_name,mode = 'w',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\"])\n",
    "        writer.writerows('./../data/outtweets')\n",
    "        print(\"CSV Was Created\")\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the function by passing the username we want to scrape. By searching specific organizations during Hurricane Sandy, we were able to obtain user names such as _\"SandyAid\", \"Fema\", \"HurricaneSandyhelp\"_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass in the username of the account you want to scrape ()\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':      \n",
    "     get_all_tweets(\"SandyAid\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few trials, we decided to only move forward with the collection of tweets from `SandyAid` because its tweets had the least amount of noise in them. `FEMA` tweets and `HurricaneSandyHelp` tweets had too much noise, represented by people talking _about_ Hurricane Sandy itself and not asking for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TwitterScraper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code block below was used to scrape tweets with Twitterscraper**\n",
    "1. Created a list of major cities we wanted to target along the east coast after researching from historical data of the location where Hurricane Sandy hit. \n",
    "2. Querying each city within 10 mile radius with key words such as _rescue_, _help_, _hurricanesandy_ during time period from October 1st through November 30th in 2012. \n",
    "3. Storing information such as tweet, ID, Username, location and timestamp in a data frame.\n",
    "4. Combining all data frames into one single one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "```python \n",
    "#creating a list of cities we want to scrape \n",
    "cities_list = ['Boston', 'Philadelphia', 'Providence','Washington DC', \n",
    "               'Buffalo', 'Toronto', 'Montreal','Richmond', 'Long Beach']\n",
    "\n",
    "#for loop to interate through our city list and scrape Twitter for a selection of targeted key words\n",
    "for city in cities_list:\n",
    "\n",
    "    #Creating empty data frame\n",
    "    df_tweets = pd.DataFrame(columns=['id','text','timestamp','user','location'])\n",
    "\n",
    "\n",
    "    #Querying tweets \n",
    "    tweet_list = query_tweets(f'\"rescue\" OR \"RESCUE\" OR \"HELP\" OR \"urgent rescue\" \n",
    "                              OR \"urgent help\" OR \"help needed\" OR  \n",
    "                              \"#HurricaneSandy\" OR \"#Hurricanesandyhelp\" \n",
    "                              -filter:retweets near:\"{city}\" within:10mi',\n",
    "                              \n",
    "                              #ADD TIMESTAMP\n",
    "                              begindate = datetime.date(2012,10,1),\n",
    "                              enddate = datetime.date(2012,11,30),\n",
    "                              poolsize = 10)\n",
    "\n",
    "    # Extract features of tweets to populate dataframe:\n",
    "    for row, tweet in enumerate(tweet_list):\n",
    "        df_tweets.loc[row,'id'] = tweet.user_id\n",
    "        df_tweets.loc[row,'text'] = tweet.text\n",
    "        df_tweets.loc[row,'timestamp'] = tweet.timestamp\n",
    "        df_tweets.loc[row,'user'] = tweet.username\n",
    "        df_tweets.loc[row,'location'] = city\n",
    "\n",
    "    df_tweets.to_csv(f'../data/tweets/{city}_tweets.csv', index=False)\n",
    "\n",
    "#reading in and combining all the tweet dataframes\n",
    "def read_and_combine_df(cities_list):\n",
    "  \n",
    "    #empty data frame \n",
    "    df_tweets = pd.DataFrame(columns=['id','text','timestamp','user','location'])\n",
    "\n",
    "    #looping through each city\n",
    "    for city in cities_list: \n",
    "        print(city)\n",
    "        city_df = pd.read_csv(f\"../data/tweets/{city}_tweets.csv\")\n",
    "\n",
    "        #combining data frames\n",
    "        df_tweets = pd.concat([df_tweets, city_df])\n",
    "        \n",
    "    return df_tweets\n",
    "\n",
    "#Combining all data frames into a single one\n",
    "df_1 = read_and_combine_df(cities_list)\n",
    "\n",
    "#Saving data frame as CSV file\n",
    "df_1.to_csv(path_or_buf='../data/df_1.csv', index=False)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TwitterScraper we were able to obtain tweets within 10 miles of radius of the following cities: \n",
    "- Boston\n",
    "- Philadelphia\n",
    "- Providence\n",
    "- Washington DC \n",
    "- Buffalo\n",
    "- Toronto\n",
    "- Montreal\n",
    "- Richmond\n",
    "- Long Beach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we have two data frames, one from the Twitter API and one from TwitterScraper. These two data frames are going to be combined in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in TwitterScraper file \n",
    "df = pd.read_csv('../data/tweets/SandyAid_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine these two data frames we have to add additional columns to the `SandyAid_twees.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename column\n",
    "df.rename(columns={'created_at': 'timestamp'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addition location for New York City tweets\n",
    "df['location'] = \"New York City\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding missing column\n",
    "df['user'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving it to a another data frame\n",
    "df.to_csv('../data/df_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining both data frames\n",
    "df_1 = pd.read_csv('../data/df_1.csv')\n",
    "df_2 = pd.read_csv('../data/df_2.csv')\n",
    "df = pd.concat([df_1, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving combined data frames\n",
    "df.to_csv('../data/df_new.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
