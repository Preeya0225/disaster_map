{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "\n",
    "Author: Preeya Sawadmanod \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we clean the collected tweets by removing the following:\n",
    "\n",
    "- Duplicates\n",
    "- Null values\n",
    "- Punctuation\n",
    "- URL \n",
    "- _@_ Retweet tags\n",
    "- Stop words\n",
    "\n",
    "Additionally, we convert our timestamp to a datetime object. After the conversion we use the timestamp to further reduce our data by limiting tweets from 27th October 2012 to 13th November 2012. This way we eliminate as much noise as possible, because people were tweeting about Hurricane Sandy prior and after the disaster, but not to ask for help. \n",
    "\n",
    "## Table of Contents \n",
    "--- \n",
    "\n",
    "- [Import Packages](#Import-Packages)\n",
    "- [First look](#First-look)\n",
    "- [Duplicates](#Duplicates)\n",
    "- [Timestamp](#Timestamp)\n",
    "- [Steps of cleaning](#Steps-of-cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import miscellaneous\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Load additional libraries \n",
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "import regex   as re\n",
    "\n",
    "#Load packages from NLTK \n",
    "from nltk.stem         import WordNetLemmatizer\n",
    "from nltk.tokenize     import RegexpTokenizer\n",
    "from nltk.corpus       import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>97559123</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Please come help end childhood stroke @Aly_Rai...</td>\n",
       "      <td>2012-10-18 22:52:29</td>\n",
       "      <td>tinyytinna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>165524230</td>\n",
       "      <td>Boston</td>\n",
       "      <td>@Aly_Raisman please come and support my bestfr...</td>\n",
       "      <td>2012-10-18 22:31:11</td>\n",
       "      <td>kwolinski3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>97559123</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Help end childhood stroke meet childhood strok...</td>\n",
       "      <td>2012-10-18 22:24:14</td>\n",
       "      <td>tinyytinna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>131966514</td>\n",
       "      <td>Boston</td>\n",
       "      <td>What's to eat at white hall. Someone help.</td>\n",
       "      <td>2012-10-18 22:01:27</td>\n",
       "      <td>mikeykauf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>799086546</td>\n",
       "      <td>Boston</td>\n",
       "      <td>we fuss &amp; fight then the next day we tight thi...</td>\n",
       "      <td>2012-10-18 21:55:34</td>\n",
       "      <td>NayGawd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id location                                               text  \\\n",
       "0   97559123   Boston  Please come help end childhood stroke @Aly_Rai...   \n",
       "1  165524230   Boston  @Aly_Raisman please come and support my bestfr...   \n",
       "2   97559123   Boston  Help end childhood stroke meet childhood strok...   \n",
       "3  131966514   Boston         What's to eat at white hall. Someone help.   \n",
       "4  799086546   Boston  we fuss & fight then the next day we tight thi...   \n",
       "\n",
       "             timestamp        user  \n",
       "0  2012-10-18 22:52:29  tinyytinna  \n",
       "1  2012-10-18 22:31:11  kwolinski3  \n",
       "2  2012-10-18 22:24:14  tinyytinna  \n",
       "3  2012-10-18 22:01:27   mikeykauf  \n",
       "4  2012-10-18 21:55:34     NayGawd  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading in data frame \n",
    "df = pd.read_csv('../data/df_new.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25440 rows.\n"
     ]
    }
   ],
   "source": [
    "# Get the size of data frame \n",
    "total_rows = df.shape[0]\n",
    "print(f'There are {total_rows} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           object\n",
       "location     object\n",
       "text         object\n",
       "timestamp    object\n",
       "user         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Timestamp is an object and needs to be converted into a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "location       2\n",
       "text           0\n",
       "timestamp      1\n",
       "user         907\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** We are going to drop our null values in `location` and in `timestamp` as 1 and 2 missing rows won't affect our overall result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['timestamp', 'location'], axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Boston', 'Philadelphia', 'Providence', 'Washington DC', 'Buffalo',\n",
       "       'Toronto', 'Montreal', 'Long Beach', 'Richmond', 'New York City'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['location'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: After we pulled all the data, we decided to drop the locations Toronto and Montreal, because after additional researach we discovered that hurricane Sandy did not have a significant impact that far north. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(axis = 0, index=df.loc[df['location'] == \"Toronto\"].index, inplace=True)\n",
    "df.drop(axis = 0, index=df.loc[df['location'] == \"Montreal\"].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for duplicates in text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1478 rows of duplicates\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {df['text'].duplicated().sum()} rows of duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicates\n",
    "df.drop_duplicates(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14147, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking if the rows are dropped \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamp\n",
    "\n",
    "1. Changing Timestamp into a day-time object\n",
    "2. Filtering through Timestamp of Hurricane Sandy to extract tweets during the disaster by specifying start date and end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2012-10-27'\n",
    "end_date = '2012-11-13'\n",
    "\n",
    "time_hurricane = (df['timestamp'] > start_date) & (df['timestamp'] <= end_date)\n",
    "df = df[time_hurricane]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:41:04.730461Z",
     "start_time": "2017-10-24T15:41:04.715975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5626 rows left in our data.\n"
     ]
    }
   ],
   "source": [
    "# Get the size of data frame \n",
    "total_rows = df.shape[0]\n",
    "print(f'There are {total_rows} rows left in our data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Steps\n",
    "\n",
    "We are going to \"tokenize\" and \"lemmatize\" our data. Tokenizing splits it up into distinct word-sized chunks. Lemmatization reduces words into their linguistic base without losing context. We avoided stemming because we believed it would cost us useful context. For instance long words such as emergency lemmatize to \"emergency\" and not \"emerge\", as would be the case with stemming. \n",
    "\n",
    "During this process:\n",
    "\n",
    "- All strings will be converted to lower case\n",
    "- All punctuation will be removed\n",
    "- All numeric digits will be removed\n",
    "\n",
    "Additionally, all single letters, stop words, and words which bear no meaning in our context such as `http` or `www` will also be removed with the function we have created below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "clean_words = []\n",
    "\n",
    "#Iterating through each row\n",
    "for rows in df['text']:\n",
    "    \n",
    "    #Removing URL in each tweet\n",
    "    result_url = re.sub(r\"http\\S+\", \"\", rows)\n",
    "    \n",
    "    #Removing Retweet in each tweet\n",
    "    result_tweet = re.sub(r\"@\\S+\", \"\", result_url)\n",
    "\n",
    "    #Instantiate Tokenizer with Regex specifying to get full words but exclude digits\n",
    "    tokenizer = RegexpTokenizer(r'\\b[^\\d\\W]+\\b')\n",
    "    \n",
    "    #Run Tokenizer on each row and converting each row into lower case \n",
    "    rows_token = tokenizer.tokenize(result_tweet.lower())\n",
    "    \n",
    "    #Lemmatize words\n",
    "    words = [lemmatizer.lemmatize(token) for token in rows_token]\n",
    "    \n",
    "    #Convert the stop words to a set.\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    #Remove stop words.\n",
    "    meaningful_words_1 = [word for word in words if word not in stops]\n",
    "    \n",
    "    #Elinimating words that are only 1 or 2 syllable\n",
    "    meaningful_words_2 = [word for word in meaningful_words_1 if len(word) > 2]\n",
    "    \n",
    "    #Removing words such as http, com, b, rt \n",
    "    words_to_remove = [\"https\", \"http\", \"www\", \"html\", \"com\", \"occupysandy\", \"b\", \"rt\", \"ave\"]\n",
    "    meaningful_words_3 = [word for word in meaningful_words_2 if word not in words_to_remove]\n",
    "\n",
    "    #Joining the words back into one string separated by space, \n",
    "    clean_word = (\" \".join(meaningful_words_3))\n",
    "    \n",
    "    #Saving it into a list\n",
    "    clean_words.append(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>59259141</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Miles Davis to the rescue so</td>\n",
       "      <td>2012-11-11 23:52:07</td>\n",
       "      <td>diediesays</td>\n",
       "      <td>mile davis rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1101</td>\n",
       "      <td>516097524</td>\n",
       "      <td>Boston</td>\n",
       "      <td>@VINNYGUADAGNINO Happy birthday man , really l...</td>\n",
       "      <td>2012-11-11 23:06:45</td>\n",
       "      <td>abdallaxiv</td>\n",
       "      <td>happy birthday man really like effort help res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1102</td>\n",
       "      <td>17152345</td>\n",
       "      <td>Boston</td>\n",
       "      <td>How You Can Help Sandy's Victims http://bit.ly...</td>\n",
       "      <td>2012-11-11 22:15:20</td>\n",
       "      <td>02458</td>\n",
       "      <td>help sandy victim via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1103</td>\n",
       "      <td>460583099</td>\n",
       "      <td>Boston</td>\n",
       "      <td>When karma finally hits you in the face, ill b...</td>\n",
       "      <td>2012-11-11 19:37:42</td>\n",
       "      <td>andreap_426</td>\n",
       "      <td>karma finally hit face ill case need help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1104</td>\n",
       "      <td>17246073</td>\n",
       "      <td>Boston</td>\n",
       "      <td>How You Can Help Sandy's Victims http://bit.ly...</td>\n",
       "      <td>2012-11-11 18:15:27</td>\n",
       "      <td>02461</td>\n",
       "      <td>help sandy victim via</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id location                                               text  \\\n",
       "1100   59259141   Boston                       Miles Davis to the rescue so   \n",
       "1101  516097524   Boston  @VINNYGUADAGNINO Happy birthday man , really l...   \n",
       "1102   17152345   Boston  How You Can Help Sandy's Victims http://bit.ly...   \n",
       "1103  460583099   Boston  When karma finally hits you in the face, ill b...   \n",
       "1104   17246073   Boston  How You Can Help Sandy's Victims http://bit.ly...   \n",
       "\n",
       "               timestamp         user  \\\n",
       "1100 2012-11-11 23:52:07   diediesays   \n",
       "1101 2012-11-11 23:06:45   abdallaxiv   \n",
       "1102 2012-11-11 22:15:20        02458   \n",
       "1103 2012-11-11 19:37:42  andreap_426   \n",
       "1104 2012-11-11 18:15:27        02461   \n",
       "\n",
       "                                                  clean  \n",
       "1100                                  mile davis rescue  \n",
       "1101  happy birthday man really like effort help res...  \n",
       "1102                              help sandy victim via  \n",
       "1103          karma finally hit face ill case need help  \n",
       "1104                              help sandy victim via  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving clean text in different columsn\n",
    "df.loc[:,'clean'] = clean_words\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving clean data\n",
    "df.to_csv('../data/df_clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
